{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from wrapper import OASIS\n",
    "from split import split_data\n",
    "\n",
    "scans_home = 'data/scans'\n",
    "labels_file = 'data/OASIS3_MRID2Label_052918.csv'\n",
    "stats_filepath = 'outputs_resnet.txt'\n",
    "n_classes = 3\n",
    "freeze_layers = False\n",
    "start_freeze_layer = 'Mixed_5d'\n",
    "use_parallel = True\n",
    "vision_model = torchvision.models.resnet50()\n",
    "\n",
    "loss_weights = torch.tensor([1.,4.2, 16.5])\n",
    "if torch.cuda.is_available():\n",
    "    loss_weights = loss_weights.cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "optimizer_type = torch.optim.Adam\n",
    "lr_scheduler_type = optim.lr_scheduler.StepLR\n",
    "num_epochs = 10\n",
    "best_model_filepath = None\n",
    "load_model_filepath = None\n",
    "#load_model_filepath = 'model_best.pth.tar'\n",
    "\n",
    "def get_counts(filename_labels):\n",
    "    counts = [0]*3\n",
    "    for filename, label in filename_labels:\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, datasets, dataset_sizes, criterion, optimizer, scheduler, use_gpu, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_f1_score = 0.0\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # list of models from all epochs\n",
    "    model_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                    model = model.cuda()\n",
    "                else:\n",
    "                    input = Variable(inputs)\n",
    "                    labels = Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                if type(outputs) == tuple:\n",
    "                    outputs, _ = outputs\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.item() / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            with open(stats_filepath, 'a') as f:\n",
    "                f.write('Epoch {} {} Loss: {:.4f} Acc: {:.4f}\\n'.format(epoch, phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                predictions = evaluate_model(model, dataloaders['val'], dataset_sizes['val'], use_cuda)\n",
    "                true_y = [y for img, y in datasets['val']]\n",
    "                f1 = f1_score(true_y, predictions, average = 'macro')\n",
    "                all_f1s = f1_score(true_y, predictions, average = None)\n",
    "                \n",
    "                # print f1 score and write to file\n",
    "                print('macro f1_score: {:.4f}'.format(f1))\n",
    "                print('all f1_scores: {}'.format(str(all_f1s)))\n",
    "                with open(stats_filepath, 'a') as f:\n",
    "                    f.write('Epoch {} macro f1_score = {:.4f} \\n'.format(epoch, f1))\n",
    "                    f.write('all f1_scores: {} \\n'.format(str(all_f1s)))\n",
    "                \n",
    "                #update epoch acc\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    \n",
    "                # update best model based on f1_score\n",
    "                if f1 > best_f1_score:\n",
    "                    best_f1_score = f1\n",
    "                    best_model_wts = model.state_dict()\n",
    "\n",
    "                    state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "                    if best_model_filepath is not None:\n",
    "                        torch.save(state, best_model_filepath)\n",
    "        \n",
    "        model_list.append(copy.deepcopy(model))\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    with open(stats_filepath, 'a') as f:\n",
    "        f.write('Best val Acc: {:4f}\\n'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model_list, model\n",
    "\n",
    "\n",
    "def evaluate_model(model, testset_loader, test_size, use_gpu):\n",
    "    model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "    predictions = []\n",
    "    # Iterate over data\n",
    "    for inputs, labels in tqdm(testset_loader):\n",
    "        # TODO: wrap them in Variable?\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        if type(outputs) == tuple:\n",
    "            outputs, _ = outputs\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        predictions.extend(preds.tolist())\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def load_saved_model(filepath, model, optimizer=None):\n",
    "    state = torch.load(filepath)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    # Only need to load optimizer if you are going to resume training on the model\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels is 2107\n",
      "num filenames is 2193\n",
      "num experiments is 1950\n",
      "counts per class: [1536, 322, 92]\n",
      "train filenames size:  1365\n",
      "validation filenames size:  292\n",
      "test filenames size:  293\n",
      "label counts for training set:  [1075, 225, 65]\n",
      "label counts for validation set:  [230, 48, 14]\n",
      "label counts for test set:  [231, 49, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'fc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2b6165876328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Since imagenet has 1000 classes, we need to change our last layer according to the number of classes we have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mvision_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 532\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'fc'"
     ]
    }
   ],
   "source": [
    "train_filenames, val_filenames, test_filenames = split_data(scans_home, labels_file)\n",
    "print('train filenames size: ', len(train_filenames))\n",
    "print('validation filenames size: ', len(val_filenames))\n",
    "print('test filenames size: ', len(test_filenames))\n",
    "print('label counts for training set: ', get_counts(train_filenames))\n",
    "print('label counts for validation set: ', get_counts(val_filenames))\n",
    "print('label counts for test set: ', get_counts(test_filenames))\n",
    "\n",
    "train_dataset = OASIS(train_filenames, input_size = 224)\n",
    "val_dataset = OASIS(val_filenames, input_size = 224)\n",
    "test_dataset = OASIS(test_filenames, input_size = 224)\n",
    "# print([y for img, y in train_dataset])\n",
    "# print([y for img, y in val_dataset])\n",
    "# print([y for img, y in test_dataset])\n",
    "\n",
    "#print out a sample image shape\n",
    "'''image_array, label = train_dataset[4]\n",
    "print(image_array.shape)'''\n",
    "# print('training dataset size: ', len(train_dataset))\n",
    "# print('validation dataset size: ', len(val_dataset))\n",
    "# print('test dataset size: ', len(test_dataset))\n",
    "trainset_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "valset_loader = DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=8)\n",
    "testset_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=8)\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Since imagenet has 1000 classes, we need to change our last layer according to the number of classes we have\n",
    "n_features = vision_model.fc.in_features\n",
    "vision_model.fc = nn.Linear(n_features, n_classes)\n",
    "\n",
    "# Freeze layers if freeze_layer is True\n",
    "for i, param in vision_model.named_parameters():\n",
    "    if freeze_layers:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "if freeze_layers:\n",
    "    ct = []\n",
    "    for name, child in vision_model.named_children():\n",
    "        #if name == 'fc':\n",
    "        if start_freeze_layer in ct:\n",
    "            for params in child.parameters():\n",
    "                params.requires_grad = True\n",
    "        ct.append(name)\n",
    "        \n",
    "# He initialization\n",
    "def init_weights(m):\n",
    "    # if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
    "    if m.requires_grad:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "# To view which layers are freezed and which layers are not freezed:\n",
    "for name, child in vision_model.named_children():\n",
    "    for name_2, params in child.named_parameters():\n",
    "        print(name_2, params.requires_grad)\n",
    "\n",
    "if use_parallel:\n",
    "    print(\"[Using all the available GPUs]\")\n",
    "    vision_model = nn.DataParallel(vision_model, device_ids=[0, 1])\n",
    "\n",
    "dataloaders = {'train': trainset_loader, 'val': valset_loader}\n",
    "datasets = {'train': train_dataset, 'val': val_dataset}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "optimizable_params = [param for param in vision_model.parameters() if param.requires_grad]\n",
    "optimizer = optimizer_type(optimizable_params, lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler_type(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# If we want to load a model with saved parameters\n",
    "if load_model_filepath is not None:\n",
    "    load_saved_model(load_model_filepath, vision_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "100%|██████████| 2045/2045 [06:33<00:00,  5.20it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1074 Acc: 0.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.43it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0935 Acc: 0.4407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.99it/s]\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3525\n",
      "all f1_scores: [0.55413504 0.30368764 0.19961612]\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [06:29<00:00,  5.26it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1019 Acc: 0.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.44it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0826 Acc: 0.6610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.95it/s]\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3407\n",
      "all f1_scores: [0.81564576 0.         0.20651311]\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [06:28<00:00,  5.26it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1004 Acc: 0.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.34it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0854 Acc: 0.6511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.93it/s]\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3447\n",
      "all f1_scores: [0.80823338 0.05868545 0.16710183]\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1482/2045 [04:41<01:46,  5.26it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 2045/2045 [06:27<00:00,  5.27it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0981 Acc: 0.6325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.43it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0802 Acc: 0.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 16.08it/s]\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3982\n",
      "all f1_scores: [0.80839661 0.16725979 0.21905805]\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [06:27<00:00,  5.28it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0965 Acc: 0.6499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.48it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0989 Acc: 0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.97it/s]\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3796\n",
      "all f1_scores: [0.656      0.27703399 0.20579268]\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [06:28<00:00,  5.27it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0965 Acc: 0.6530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.41it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0689 Acc: 0.7578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 16.06it/s]\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3616\n",
      "all f1_scores: [0.86766857 0.07188161 0.14532872]\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [06:27<00:00,  5.27it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0945 Acc: 0.6596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.59it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0865 Acc: 0.6230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 16.14it/s]\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.4228\n",
      "all f1_scores: [0.77915793 0.26687847 0.22222222]\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [06:27<00:00,  5.28it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0915 Acc: 0.6759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.47it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0805 Acc: 0.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.93it/s]\n",
      "  0%|          | 0/2045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.4602\n",
      "all f1_scores: [0.80633147 0.29901639 0.27515924]\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [06:27<00:00,  5.28it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0902 Acc: 0.6793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:28<00:00, 15.34it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0767 Acc: 0.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.94it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.4700\n",
      "all f1_scores: [0.82383499 0.30397727 0.28214732]\n",
      "\n",
      "Training complete in 76m 18s\n",
      "Best val Acc: 0.757825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.96it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.40      0.55      3447\n",
      "          1       0.20      0.68      0.30       720\n",
      "          2       0.17      0.25      0.20       210\n",
      "\n",
      "avg / total       0.74      0.44      0.50      4377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 16.06it/s]\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.80      0.82      3447\n",
      "          1       0.00      0.00      0.00       720\n",
      "          2       0.12      0.62      0.21       210\n",
      "\n",
      "avg / total       0.66      0.66      0.65      4377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.97it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.79      0.81      3447\n",
      "          1       0.19      0.03      0.06       720\n",
      "          2       0.10      0.46      0.17       210\n",
      "\n",
      "avg / total       0.69      0.65      0.65      4377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.95it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.78      0.81      3447\n",
      "          1       0.14      0.05      0.08       720\n",
      "          2       0.13      0.50      0.20       210\n",
      "\n",
      "avg / total       0.68      0.65      0.66      4377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.96it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.79      0.81      3447\n",
      "          1       0.23      0.13      0.17       720\n",
      "          2       0.14      0.48      0.22       210\n",
      "\n",
      "avg / total       0.70      0.66      0.67      4377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 15.98it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.52      0.66      3447\n",
      "          1       0.22      0.37      0.28       720\n",
      "          2       0.12      0.64      0.21       210\n",
      "\n",
      "avg / total       0.73      0.50      0.57      4377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 16.07it/s]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.95      0.87      3447\n",
      "          1       0.15      0.05      0.07       720\n",
      "          2       0.27      0.10      0.15       210\n",
      "\n",
      "avg / total       0.67      0.76      0.70      4377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 427/438 [00:26<00:00, 16.02it/s]"
     ]
    }
   ],
   "source": [
    "model_list, best_model = train_model(vision_model,\n",
    "                             dataloaders,\n",
    "                             datasets,\n",
    "                             dataset_sizes,\n",
    "                             criterion,\n",
    "                             optimizer,\n",
    "                             exp_lr_scheduler,\n",
    "                             use_cuda,\n",
    "                             num_epochs)\n",
    "    \n",
    "epoch = 0 \n",
    "for model in model_list:\n",
    "    predictions = evaluate_model(model, valset_loader, len(val_dataset), use_cuda)\n",
    "    true_y = [y for img, y in val_dataset]\n",
    "    report = classification_report(true_y, predictions)\n",
    "    with open(stats_filepath, 'a') as f:\n",
    "        f.write('\\n Epoch {} \\n'.format(epoch))\n",
    "        f.write(report)\n",
    "    epoch += 1\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:27<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4371\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.78      0.82      3447\n",
      "          1       0.31      0.30      0.30       720\n",
      "          2       0.19      0.54      0.28       210\n",
      "\n",
      "avg / total       0.75      0.69      0.71      4377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluate_model(best_model, testset_loader, len(test_dataset), use_cuda)\n",
    "true_y = [y for img, y in test_dataset]\n",
    "best_report = classification_report(true_y, predictions)\n",
    "\n",
    "with open(stats_filepath, 'a') as f:\n",
    "    f.write('\\n Best report \\n {}'.format(best_report))   \n",
    "    print(best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro f1 0.6728437428506062\n",
      "macro f1 0.4186048576126473\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.78      0.82      3458\n",
      "          1       0.28      0.25      0.26       718\n",
      "          2       0.12      0.35      0.18       195\n",
      "\n",
      "avg / total       0.73      0.67      0.70      4371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('micro f1', f1_score(true_y, predictions, average = 'micro'))\n",
    "print('macro f1', f1_score(true_y, predictions, average = 'macro'))\n",
    "\n",
    "print(best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is 0.6728437428506062\n",
      "\n",
      "prediction\n",
      " 0    3137\n",
      "1     657\n",
      "2     577\n",
      "dtype: int64\n",
      "\n",
      "true values \n",
      " 0    3458\n",
      "1     718\n",
      "2     195\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_array = np.array(predictions)\n",
    "true_y_array = np.array(true_y)\n",
    "test_acc = np.average(prediction_array == true_y_array)\n",
    "print(\"test accuracy is {}\".format(test_acc))\n",
    "print()\n",
    "\n",
    "import pandas as pd\n",
    "print(\"prediction\\n\", pd.Series(prediction_array).value_counts())\n",
    "print()\n",
    "print(\"true values \\n\", pd.Series(true_y_array).value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4371\n",
      "4377\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
