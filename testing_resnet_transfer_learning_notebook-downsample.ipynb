{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from wrapper import OASIS\n",
    "from split import split_data\n",
    "\n",
    "scans_home = 'data/scans'\n",
    "labels_file = 'data/OASIS3_MRID2Label_052918.csv'\n",
    "stats_filepath = 'test_downsampled_resnet_outputs.txt'\n",
    "n_classes = 3\n",
    "freeze_layers = False\n",
    "start_freeze_layer = 'Mixed_5d'\n",
    "use_parallel = True\n",
    "vision_model = torchvision.models.resnet50()\n",
    "\n",
    "loss_weights = torch.tensor([1.,1.,5.])\n",
    "if torch.cuda.is_available():\n",
    "    loss_weights = loss_weights.cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "optimizer_type = torch.optim.Adam\n",
    "lr_scheduler_type = optim.lr_scheduler.StepLR\n",
    "num_epochs = 20\n",
    "best_model_filepath = None\n",
    "load_model_filepath = None\n",
    "#load_model_filepath = 'model_best.pth.tar'\n",
    "\n",
    "def get_counts(filename_labels):\n",
    "    counts = [0]*3\n",
    "    for filename, label in filename_labels:\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, datasets, dataset_sizes, criterion, optimizer, scheduler, use_gpu, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_f1_score = 0.0\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # list of models from all epochs\n",
    "    model_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                    model = model.cuda()\n",
    "                else:\n",
    "                    input = Variable(inputs)\n",
    "                    labels = Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                if type(outputs) == tuple:\n",
    "                    outputs, _ = outputs\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = int(running_corrects) / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            with open(stats_filepath, 'a') as f:\n",
    "                f.write('Epoch {} {} Loss: {:.4f} Acc: {:.4f}\\n'.format(epoch, phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                predictions = evaluate_model(model, dataloaders['val'], dataset_sizes['val'], use_cuda)\n",
    "                true_y = [y for img, y in datasets['val']]\n",
    "                f1 = f1_score(true_y, predictions, average = 'macro')\n",
    "                all_f1s = f1_score(true_y, predictions, average = None)\n",
    "                \n",
    "                report = classification_report(true_y, predictions)                   \n",
    "                \n",
    "                # print f1 score and write to file\n",
    "                print('macro f1_score: {:.4f}'.format(f1))\n",
    "                print('all f1_scores: {}'.format(str(all_f1s)))\n",
    "                print(report)\n",
    "                with open(stats_filepath, 'a') as f:\n",
    "                    f.write('Epoch {} macro f1_score = {:.4f} \\n'.format(epoch, f1))\n",
    "                    f.write('all f1_scores: {} \\n'.format(str(all_f1s)))\n",
    "                    f.write('\\n Epoch {} \\n {} \\n\\n'.format(epoch, report))\n",
    "                \n",
    "                #update epoch acc\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    \n",
    "                # update best model based on f1_score\n",
    "                if f1 > best_f1_score:\n",
    "                    best_f1_score = f1\n",
    "                    best_model_wts = model.state_dict()\n",
    "\n",
    "                    state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "                    if best_model_filepath is not None:\n",
    "                        torch.save(state, best_model_filepath)\n",
    "        \n",
    "        model_list.append(copy.deepcopy(model))\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    with open(stats_filepath, 'a') as f:\n",
    "        f.write('Best val Acc: {:4f}\\n'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model_list, model\n",
    "\n",
    "\n",
    "def evaluate_model(model, testset_loader, test_size, use_gpu):\n",
    "    model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "    predictions = []\n",
    "    # Iterate over data\n",
    "    for inputs, labels in tqdm(testset_loader):\n",
    "        # TODO: wrap them in Variable?\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        if type(outputs) == tuple:\n",
    "            outputs, _ = outputs\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        predictions.extend(preds.tolist())\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def load_saved_model(filepath, model, optimizer=None):\n",
    "    state = torch.load(filepath)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    # Only need to load optimizer if you are going to resume training on the model\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels is 2107\n",
      "num filenames is 2193\n",
      "num experiments is 1950\n",
      "counts per class: [1536, 322, 92]\n",
      "train filenames size:  515\n",
      "validation filenames size:  110\n",
      "test filenames size:  111\n",
      "label counts for training set:  [225, 225, 65]\n",
      "label counts for validation set:  [48, 48, 14]\n",
      "label counts for test set:  [49, 49, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size:  7697\n",
      "validation dataset size:  1650\n",
      "test dataset size:  1665\n",
      "cuda\n",
      "weight True\n",
      "weight True\n",
      "bias True\n",
      "0.conv1.weight True\n",
      "0.bn1.weight True\n",
      "0.bn1.bias True\n",
      "0.conv2.weight True\n",
      "0.bn2.weight True\n",
      "0.bn2.bias True\n",
      "0.conv3.weight True\n",
      "0.bn3.weight True\n",
      "0.bn3.bias True\n",
      "0.downsample.0.weight True\n",
      "0.downsample.1.weight True\n",
      "0.downsample.1.bias True\n",
      "1.conv1.weight True\n",
      "1.bn1.weight True\n",
      "1.bn1.bias True\n",
      "1.conv2.weight True\n",
      "1.bn2.weight True\n",
      "1.bn2.bias True\n",
      "1.conv3.weight True\n",
      "1.bn3.weight True\n",
      "1.bn3.bias True\n",
      "2.conv1.weight True\n",
      "2.bn1.weight True\n",
      "2.bn1.bias True\n",
      "2.conv2.weight True\n",
      "2.bn2.weight True\n",
      "2.bn2.bias True\n",
      "2.conv3.weight True\n",
      "2.bn3.weight True\n",
      "2.bn3.bias True\n",
      "0.conv1.weight True\n",
      "0.bn1.weight True\n",
      "0.bn1.bias True\n",
      "0.conv2.weight True\n",
      "0.bn2.weight True\n",
      "0.bn2.bias True\n",
      "0.conv3.weight True\n",
      "0.bn3.weight True\n",
      "0.bn3.bias True\n",
      "0.downsample.0.weight True\n",
      "0.downsample.1.weight True\n",
      "0.downsample.1.bias True\n",
      "1.conv1.weight True\n",
      "1.bn1.weight True\n",
      "1.bn1.bias True\n",
      "1.conv2.weight True\n",
      "1.bn2.weight True\n",
      "1.bn2.bias True\n",
      "1.conv3.weight True\n",
      "1.bn3.weight True\n",
      "1.bn3.bias True\n",
      "2.conv1.weight True\n",
      "2.bn1.weight True\n",
      "2.bn1.bias True\n",
      "2.conv2.weight True\n",
      "2.bn2.weight True\n",
      "2.bn2.bias True\n",
      "2.conv3.weight True\n",
      "2.bn3.weight True\n",
      "2.bn3.bias True\n",
      "3.conv1.weight True\n",
      "3.bn1.weight True\n",
      "3.bn1.bias True\n",
      "3.conv2.weight True\n",
      "3.bn2.weight True\n",
      "3.bn2.bias True\n",
      "3.conv3.weight True\n",
      "3.bn3.weight True\n",
      "3.bn3.bias True\n",
      "0.conv1.weight True\n",
      "0.bn1.weight True\n",
      "0.bn1.bias True\n",
      "0.conv2.weight True\n",
      "0.bn2.weight True\n",
      "0.bn2.bias True\n",
      "0.conv3.weight True\n",
      "0.bn3.weight True\n",
      "0.bn3.bias True\n",
      "0.downsample.0.weight True\n",
      "0.downsample.1.weight True\n",
      "0.downsample.1.bias True\n",
      "1.conv1.weight True\n",
      "1.bn1.weight True\n",
      "1.bn1.bias True\n",
      "1.conv2.weight True\n",
      "1.bn2.weight True\n",
      "1.bn2.bias True\n",
      "1.conv3.weight True\n",
      "1.bn3.weight True\n",
      "1.bn3.bias True\n",
      "2.conv1.weight True\n",
      "2.bn1.weight True\n",
      "2.bn1.bias True\n",
      "2.conv2.weight True\n",
      "2.bn2.weight True\n",
      "2.bn2.bias True\n",
      "2.conv3.weight True\n",
      "2.bn3.weight True\n",
      "2.bn3.bias True\n",
      "3.conv1.weight True\n",
      "3.bn1.weight True\n",
      "3.bn1.bias True\n",
      "3.conv2.weight True\n",
      "3.bn2.weight True\n",
      "3.bn2.bias True\n",
      "3.conv3.weight True\n",
      "3.bn3.weight True\n",
      "3.bn3.bias True\n",
      "4.conv1.weight True\n",
      "4.bn1.weight True\n",
      "4.bn1.bias True\n",
      "4.conv2.weight True\n",
      "4.bn2.weight True\n",
      "4.bn2.bias True\n",
      "4.conv3.weight True\n",
      "4.bn3.weight True\n",
      "4.bn3.bias True\n",
      "5.conv1.weight True\n",
      "5.bn1.weight True\n",
      "5.bn1.bias True\n",
      "5.conv2.weight True\n",
      "5.bn2.weight True\n",
      "5.bn2.bias True\n",
      "5.conv3.weight True\n",
      "5.bn3.weight True\n",
      "5.bn3.bias True\n",
      "0.conv1.weight True\n",
      "0.bn1.weight True\n",
      "0.bn1.bias True\n",
      "0.conv2.weight True\n",
      "0.bn2.weight True\n",
      "0.bn2.bias True\n",
      "0.conv3.weight True\n",
      "0.bn3.weight True\n",
      "0.bn3.bias True\n",
      "0.downsample.0.weight True\n",
      "0.downsample.1.weight True\n",
      "0.downsample.1.bias True\n",
      "1.conv1.weight True\n",
      "1.bn1.weight True\n",
      "1.bn1.bias True\n",
      "1.conv2.weight True\n",
      "1.bn2.weight True\n",
      "1.bn2.bias True\n",
      "1.conv3.weight True\n",
      "1.bn3.weight True\n",
      "1.bn3.bias True\n",
      "2.conv1.weight True\n",
      "2.bn1.weight True\n",
      "2.bn1.bias True\n",
      "2.conv2.weight True\n",
      "2.bn2.weight True\n",
      "2.bn2.bias True\n",
      "2.conv3.weight True\n",
      "2.bn3.weight True\n",
      "2.bn3.bias True\n",
      "weight True\n",
      "bias True\n",
      "[Using all the available GPUs]\n"
     ]
    }
   ],
   "source": [
    "train_filenames, val_filenames, test_filenames = split_data(scans_home, labels_file, balanced = True)\n",
    "print('train filenames size: ', len(train_filenames))\n",
    "print('validation filenames size: ', len(val_filenames))\n",
    "print('test filenames size: ', len(test_filenames))\n",
    "print('label counts for training set: ', get_counts(train_filenames))\n",
    "print('label counts for validation set: ', get_counts(val_filenames))\n",
    "print('label counts for test set: ', get_counts(test_filenames))\n",
    "\n",
    "train_dataset = OASIS(train_filenames, input_size = 224)\n",
    "val_dataset = OASIS(val_filenames, input_size = 224)\n",
    "test_dataset = OASIS(test_filenames, input_size = 224)\n",
    "# print([y for img, y in train_dataset])\n",
    "# print([y for img, y in val_dataset])\n",
    "# print([y for img, y in test_dataset])\n",
    "\n",
    "#print out a sample image shape\n",
    "'''image_array, label = train_dataset[4]\n",
    "print(image_array.shape)'''\n",
    "print('training dataset size: ', len(train_dataset))\n",
    "print('validation dataset size: ', len(val_dataset))\n",
    "print('test dataset size: ', len(test_dataset))\n",
    "\n",
    "# utilize the distribution of classes to probabilistically sample\n",
    "# batch_size = 20\n",
    "# class_sample_count = np.array([1000, 500, 250]) #distribution of the class examples\n",
    "# weights = 1./ class_sample_count\n",
    "# sample_weights = np.array([weights[t] for img, t in train_dataset])\n",
    "# sample_weights = torch.from_numpy(sample_weights).double()\n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "trainset_loader = DataLoader(train_dataset, batch_size=20, shuffle=False, num_workers=8)\n",
    "valset_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, num_workers=8)\n",
    "testset_loader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=8)\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Since imagenet has 1000 classes, we need to change our last layer according to the number of classes we have\n",
    "n_features = vision_model.fc.in_features\n",
    "vision_model.fc = nn.Linear(n_features, n_classes)\n",
    "\n",
    "# Freeze layers if freeze_layer is True\n",
    "for i, param in vision_model.named_parameters():\n",
    "    if freeze_layers:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "if freeze_layers:\n",
    "    ct = []\n",
    "    for name, child in vision_model.named_children():\n",
    "        #if name == 'fc':\n",
    "        if start_freeze_layer in ct:\n",
    "            for params in child.parameters():\n",
    "                params.requires_grad = True\n",
    "        ct.append(name)\n",
    "        \n",
    "# He initialization\n",
    "def init_weights(m):\n",
    "    # if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
    "    if m.requires_grad:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "# To view which layers are freezed and which layers are not freezed:\n",
    "for name, child in vision_model.named_children():\n",
    "    for name_2, params in child.named_parameters():\n",
    "        print(name_2, params.requires_grad)\n",
    "\n",
    "if use_parallel:\n",
    "    print(\"[Using all the available GPUs]\")\n",
    "    vision_model = nn.DataParallel(vision_model, device_ids=[0, 1])\n",
    "\n",
    "dataloaders = {'train': trainset_loader, 'val': valset_loader}\n",
    "datasets = {'train': train_dataset, 'val': val_dataset}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "optimizable_params = [param for param in vision_model.parameters() if param.requires_grad]\n",
    "optimizer = optimizer_type(optimizable_params, lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler_type(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# If we want to load a model with saved parameters\n",
    "if load_model_filepath is not None:\n",
    "    load_saved_model(load_model_filepath, vision_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "100%|██████████| 385/385 [02:03<00:00,  3.11it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0713 Acc: 0.3915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.00it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1384 Acc: 0.3794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.21it/s]\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3033\n",
      "all f1_scores: [0.         0.53933865 0.37046005]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       720\n",
      "          1       0.46      0.66      0.54       720\n",
      "          2       0.25      0.73      0.37       210\n",
      "\n",
      "avg / total       0.23      0.38      0.28      1650\n",
      "\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.20it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0574 Acc: 0.3786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.98it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0645 Acc: 0.3642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.17it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.3305\n",
      "all f1_scores: [0.45385149 0.36134454 0.17638266]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.45      0.45       720\n",
      "          1       0.46      0.30      0.36       720\n",
      "          2       0.13      0.28      0.18       210\n",
      "\n",
      "avg / total       0.41      0.36      0.38      1650\n",
      "\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.20it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0554 Acc: 0.3877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.97it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0539 Acc: 0.4630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:08<00:00,  9.23it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2933\n",
      "all f1_scores: [0.60325048 0.26774848 0.00900901]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.88      0.60       720\n",
      "          1       0.50      0.18      0.27       720\n",
      "          2       0.08      0.00      0.01       210\n",
      "\n",
      "avg / total       0.43      0.46      0.38      1650\n",
      "\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.20it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0553 Acc: 0.3969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.05it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0530 Acc: 0.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:08<00:00,  9.23it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2558\n",
      "all f1_scores: [0.60822898 0.15925059 0.        ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.94      0.61       720\n",
      "          1       0.51      0.09      0.16       720\n",
      "          2       0.00      0.00      0.00       210\n",
      "\n",
      "avg / total       0.42      0.45      0.33      1650\n",
      "\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0551 Acc: 0.3942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.00it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0532 Acc: 0.4442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:08<00:00,  9.22it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2593\n",
      "all f1_scores: [0.59306569 0.18485523 0.        ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.90      0.59       720\n",
      "          1       0.47      0.12      0.18       720\n",
      "          2       0.00      0.00      0.00       210\n",
      "\n",
      "avg / total       0.40      0.44      0.34      1650\n",
      "\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [01:59<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0544 Acc: 0.3918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.96it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0530 Acc: 0.4558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.22it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2750\n",
      "all f1_scores: [0.60018553 0.2248394  0.        ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.90      0.60       720\n",
      "          1       0.49      0.15      0.22       720\n",
      "          2       0.00      0.00      0.00       210\n",
      "\n",
      "avg / total       0.41      0.46      0.36      1650\n",
      "\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0540 Acc: 0.4016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.03it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0527 Acc: 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.20it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2864\n",
      "all f1_scores: [0.61417323 0.24489796 0.        ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.92      0.61       720\n",
      "          1       0.54      0.16      0.24       720\n",
      "          2       0.00      0.00      0.00       210\n",
      "\n",
      "avg / total       0.44      0.47      0.37      1650\n",
      "\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0530 Acc: 0.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.04it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0525 Acc: 0.4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.17it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2572\n",
      "all f1_scores: [0.60938884 0.11221945 0.05      ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.96      0.61       720\n",
      "          1       0.55      0.06      0.11       720\n",
      "          2       0.20      0.03      0.05       210\n",
      "\n",
      "avg / total       0.46      0.45      0.32      1650\n",
      "\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0528 Acc: 0.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.10it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0520 Acc: 0.4442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:08<00:00,  9.22it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2818\n",
      "all f1_scores: [0.60448102 0.13793103 0.10294118]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.92      0.60       720\n",
      "          1       0.48      0.08      0.14       720\n",
      "          2       0.23      0.07      0.10       210\n",
      "\n",
      "avg / total       0.43      0.44      0.34      1650\n",
      "\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [01:59<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0527 Acc: 0.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.07it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0516 Acc: 0.4321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:08<00:00,  9.26it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2536\n",
      "all f1_scores: [0.6089262  0.         0.15189873]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.96      0.61       720\n",
      "          1       0.00      0.00      0.00       720\n",
      "          2       0.23      0.11      0.15       210\n",
      "\n",
      "avg / total       0.22      0.43      0.29      1650\n",
      "\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0527 Acc: 0.4321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.06it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0514 Acc: 0.4309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.18it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2437\n",
      "all f1_scores: [0.60534385 0.00824176 0.11764706]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.96      0.61       720\n",
      "          1       0.38      0.00      0.01       720\n",
      "          2       0.22      0.08      0.12       210\n",
      "\n",
      "avg / total       0.38      0.43      0.28      1650\n",
      "\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0526 Acc: 0.4332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.02it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0511 Acc: 0.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.19it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2467\n",
      "all f1_scores: [0.60035682 0.01069519 0.12903226]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.93      0.60       720\n",
      "          1       0.14      0.01      0.01       720\n",
      "          2       0.20      0.10      0.13       210\n",
      "\n",
      "avg / total       0.28      0.42      0.28      1650\n",
      "\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0525 Acc: 0.4332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.99it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0510 Acc: 0.4242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.20it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2637\n",
      "all f1_scores: [0.60853603 0.03026482 0.15243902]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.92      0.61       720\n",
      "          1       0.16      0.02      0.03       720\n",
      "          2       0.21      0.12      0.15       210\n",
      "\n",
      "avg / total       0.30      0.42      0.30      1650\n",
      "\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0526 Acc: 0.4343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.04it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0513 Acc: 0.4291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.22it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2577\n",
      "all f1_scores: [0.60736748 0.02638522 0.13924051]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.94      0.61       720\n",
      "          1       0.26      0.01      0.03       720\n",
      "          2       0.21      0.10      0.14       210\n",
      "\n",
      "avg / total       0.34      0.43      0.29      1650\n",
      "\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0522 Acc: 0.4406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.86it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0507 Acc: 0.4345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.16it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2498\n",
      "all f1_scores: [0.61108647 0.03821656 0.1       ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.96      0.61       720\n",
      "          1       0.23      0.02      0.04       720\n",
      "          2       0.26      0.06      0.10       210\n",
      "\n",
      "avg / total       0.33      0.43      0.30      1650\n",
      "\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0520 Acc: 0.4461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.04it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0504 Acc: 0.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:08<00:00,  9.23it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2669\n",
      "all f1_scores: [0.61698284 0.1147541  0.06896552]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.95      0.62       720\n",
      "          1       0.37      0.07      0.11       720\n",
      "          2       0.36      0.04      0.07       210\n",
      "\n",
      "avg / total       0.41      0.45      0.33      1650\n",
      "\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0519 Acc: 0.4504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.97it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0502 Acc: 0.4509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.15it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2601\n",
      "all f1_scores: [0.61891644 0.15198238 0.00934579]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.94      0.62       720\n",
      "          1       0.37      0.10      0.15       720\n",
      "          2       0.25      0.00      0.01       210\n",
      "\n",
      "avg / total       0.39      0.45      0.34      1650\n",
      "\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.21it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0518 Acc: 0.4530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.01it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0500 Acc: 0.4576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.21it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2722\n",
      "all f1_scores: [0.61982317 0.18743344 0.00943396]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.93      0.62       720\n",
      "          1       0.40      0.12      0.19       720\n",
      "          2       0.50      0.00      0.01       210\n",
      "\n",
      "avg / total       0.44      0.46      0.35      1650\n",
      "\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.20it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0517 Acc: 0.4559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.02it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0499 Acc: 0.4648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.21it/s]\n",
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2803\n",
      "all f1_scores: [0.62517548 0.20631579 0.00938967]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.93      0.63       720\n",
      "          1       0.43      0.14      0.21       720\n",
      "          2       0.33      0.00      0.01       210\n",
      "\n",
      "avg / total       0.43      0.46      0.36      1650\n",
      "\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [02:00<00:00,  3.20it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0516 Acc: 0.4564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  8.94it/s]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0498 Acc: 0.4642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:09<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1_score: 0.2789\n",
      "all f1_scores: [0.62348555 0.20360551 0.00947867]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.93      0.62       720\n",
      "          1       0.43      0.13      0.20       720\n",
      "          2       1.00      0.00      0.01       210\n",
      "\n",
      "avg / total       0.52      0.46      0.36      1650\n",
      "\n",
      "\n",
      "Training complete in 47m 53s\n",
      "Best val Acc: 0.470909\n"
     ]
    }
   ],
   "source": [
    "model_list, best_model = train_model(vision_model,\n",
    "                             dataloaders,\n",
    "                             datasets,\n",
    "                             dataset_sizes,\n",
    "                             criterion,\n",
    "                             optimizer,\n",
    "                             exp_lr_scheduler,\n",
    "                             use_cuda,\n",
    "                             num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:09<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.88      0.62       735\n",
      "          1       0.51      0.22      0.31       735\n",
      "          2       0.00      0.00      0.00       195\n",
      "\n",
      "avg / total       0.44      0.49      0.41      1665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluate_model(best_model, testset_loader, len(test_dataset), use_cuda)\n",
    "true_y = [y for img, y in test_dataset]\n",
    "best_report = classification_report(true_y, predictions)\n",
    "\n",
    "with open(stats_filepath, 'a') as f:\n",
    "    f.write('\\n Best report \\n {}'.format(best_report))   \n",
    "    print(best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro f1 0.4870870870870871\n",
      "macro f1 0.31036121673003797\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.88      0.62       735\n",
      "          1       0.51      0.22      0.31       735\n",
      "          2       0.00      0.00      0.00       195\n",
      "\n",
      "avg / total       0.44      0.49      0.41      1665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('micro f1', f1_score(true_y, predictions, average = 'micro'))\n",
    "print('macro f1', f1_score(true_y, predictions, average = 'macro'))\n",
    "\n",
    "print(best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is 0.4870870870870871\n",
      "\n",
      "prediction\n",
      " 0    1345\n",
      "1     317\n",
      "2       3\n",
      "dtype: int64\n",
      "\n",
      "true values \n",
      " 1    735\n",
      "0    735\n",
      "2    195\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_array = np.array(predictions)\n",
    "true_y_array = np.array(true_y)\n",
    "test_acc = np.average(prediction_array == true_y_array)\n",
    "print(\"test accuracy is {}\".format(test_acc))\n",
    "print()\n",
    "\n",
    "import pandas as pd\n",
    "print(\"prediction\\n\", pd.Series(prediction_array).value_counts())\n",
    "print()\n",
    "print(\"true values \\n\", pd.Series(true_y_array).value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665\n",
      "1650\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
